from typing import Any, List, Dict, Union, Optional
from langchain.chains import LLMChain
from llmreflect.Prompt.BasicPrompt import BasicPrompt
from langchain.base_language import BaseLanguageModel
from abc import ABC, abstractclassmethod
from llmreflect.Retriever.BasicRetriever import BasicRetriever
from dataclasses import dataclass
from langchain.chat_models import ChatOpenAI
from llmreflect.Utils.log import get_logger
from llmreflect.Utils.log import openai_trace_var, check_current_openai_balance
from langchain.callbacks.base import BaseCallbackHandler, BaseCallbackManager
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.manager import CallbackManagerForChainRun
import inspect
from langchain.load.dump import dumpd
from langchain.schema import RUN_KEY, RunInfo, LLMResult

Callbacks = Optional[Union[List[BaseCallbackHandler], BaseCallbackManager]]


@dataclass
class LLM_BACKBONE_MODEL:
    """
    LLM names used for referencing
    """
    gpt_4 = "gpt-4"
    gpt_4_0314 = "gpt-4-0314"
    gpt_4_0613 = "gpt-4-0613"
    gpt_4_32k = "gpt-4-32k"
    gpt_4_32k_0314 = "gpt-4-32k-0314"
    gpt_4_32k_0613 = "gpt-4-32k-0613"
    gpt_3_5_turbo = "gpt-3.5-turbo"
    gpt_3_5_turbo_0301 = "gpt-3.5-turbo-0301"
    gpt_3_5_turbo_0613 = "gpt-3.5-turbo-0613"
    gpt_3_5_turbo_16k = "gpt-3.5-turbo-16k"
    gpt_3_5_turbo_16k_0613 = "gpt-3.5-turbo-16k-0613"
    text_ada_001 = "text-ada-001"
    ada = "ada"
    text_babbage_001 = "text-babbage-001"
    babbage = "babbage"
    text_curie_001 = "text-curie-001"
    curie = "curie"
    davinci = "davinci"
    text_davinci_003 = "text-davinci-003"
    text_davinci_002 = "text-davinci-002"
    code_davinci_002 = "code-davinci-002"
    code_davinci_001 = "code-davinci-001"
    code_cushman_002 = "code-cushman-002"
    code_cushman_001 = "code-cushman-001"


class Agent(LLMChain, ABC):
    '''
    Abstract class for agent, in this design each agent should have
    a retriever, retriever is for retrieving the final result based
    on the gross output by LLM.
    For example, a database retriever does the following job:
    extract the sql command from the llm output and then
    execute the command in the database.
    '''
    def __init__(self, prompt: BasicPrompt, llm: BaseLanguageModel):
        super().__init__(prompt=prompt.get_langchain_prompt_template(),
                         llm=llm)
        # Agent class inherit from the LLM chain class
        # Assume the attribute llm has the properties, model_name and
        # max_tokens
        object.__setattr__(self, 'retriever', None)
        object.__setattr__(self, "logger", get_logger(self.__class__.__name__))
        object.__setattr__(self, "max_output_tokens", self.llm.max_tokens)
        object.__setattr__(self, "model_name", self.llm.model_name)

    @abstractclassmethod
    def equip_retriever(self, retriever: BasicRetriever):
        # must have the function to equip a retriever
        object.__setattr__(self, 'retriever', retriever)

    def get_inputs(self) -> List[str]:
        """
        showing inputs for the prompt template being used
        Returns:
            List: a list of strings
        """
        return self.prompt.input_variables

    def predict(self, **kwargs: Any) -> str:
        """
        The llm prediction interface.
        Returns:
            str: The output / completion generated by llm.
        """
        return self._predict(inputs=kwargs, callbacks=[openai_trace_var.get()])

    def _predict(
        self,
        inputs: Union[Dict[str, Any], Any],
        return_only_outputs: bool = False,
        callbacks: Callbacks = None,
        *,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        include_run_info: bool = False,
    ) -> Dict[str, Any]:
        """Execute the chain.

        Args:
            inputs: Dictionary of inputs, or single input if chain expects
                only one param. Should contain all inputs specified in
                `Chain.input_keys` except for inputs that will be set by the
                chain's memory.
            return_only_outputs: Whether to return only outputs in the
                response. If True, only new keys generated by this chain will
                be returned. If False, both input keys and new keys generated
                by this chain will be returned. Defaults to False.
            callbacks: Callbacks to use for this chain run. These will be
                called in addition to callbacks passed to the chain during
                construction, but only these runtime callbacks will propagate
                to calls to other objects.
            tags: List of string tags to pass to all callbacks. These will be
                passed in addition to tags passed to the chain during
                construction, but only these runtime tags will propagate to
                calls to other objects.
            metadata: Optional metadata associated with the chain.
                Defaults to None
            include_run_info: Whether to include run info in the response.
                Defaults to False.

        Returns:
            A dict of named outputs. Should contain all outputs specified in
                `Chain.output_keys`.
        """
        inputs = self.prep_inputs(inputs)
        callback_manager = CallbackManager.configure(
            callbacks,
            self.callbacks,
            self.verbose,
            tags,
            self.tags,
            metadata,
            self.metadata,
        )
        new_arg_supported = inspect.signature(self._call).\
            parameters.get("run_manager")
        run_manager = callback_manager.on_chain_start(
            dumpd(self),
            inputs,
        )
        try:
            outputs = (
                self._call(inputs, run_manager=run_manager)
                if new_arg_supported
                else self._call(inputs)
            )
        except (KeyboardInterrupt, Exception) as e:
            run_manager.on_chain_error(e)
            raise e
        run_manager.on_chain_end(outputs)
        final_outputs: Dict[str, Any] = self.prep_outputs(
            inputs, outputs, return_only_outputs
        )
        if include_run_info:
            final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)
        return final_outputs[self.output_key]

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, str]:
        """call function used in _predict function

        Args:
            inputs (Dict[str, Any]): inputs prepared by `self.prep_input`
            run_manager (Optional[CallbackManagerForChainRun], optional):
                run manager provided by callback manager. Defaults to None.

        Returns:
            Dict[str, str]: llm outputs
        """
        response = self.generate([inputs], run_manager=run_manager)
        return self.create_outputs(response)[0]

    def generate(
        self,
        input_list: List[Dict[str, Any]],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> LLMResult:
        """
        The core function for generating LLM result from inputs.
        By using the "check_current_openai_balance". The generation
        will be stopped when the cost is going to exceed the budget.
        """
        prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)
        run_permit = check_current_openai_balance(
            input_prompt=prompts[0].to_string(),
            max_output_tokens=self.max_output_tokens,
            model_name=self.model_name,
            logger=self.logger)
        if run_permit:
            return self.llm.generate_prompt(
                prompts,
                stop,
                callbacks=run_manager.get_child() if run_manager else None,
                **self.llm_kwargs,
            )
        else:
            raise Exception("Budget Error: The next round text completion \
is likely to exceed the budget. LLM is forced to stop.")


class OpenAIAgent(Agent):
    '''
    Agent class specifically designed for openAI.
    '''
    def __init__(self, open_ai_key: str,
                 prompt_name: str = '',
                 max_output_tokens: int = 512,
                 temperature: float = 0.0,
                 llm_model=LLM_BACKBONE_MODEL.gpt_3_5_turbo):
        prompt = BasicPrompt.\
            load_prompt_from_json_file(prompt_name)
        llm = ChatOpenAI(temperature=temperature,
                         openai_api_key=open_ai_key,
                         model=llm_model)
        llm.max_tokens = max_output_tokens
        super().__init__(prompt=prompt,
                         llm=llm)
        object.__setattr__(self, 'retriever', None)

    @abstractclassmethod
    def equip_retriever(self, retriever: BasicRetriever):
        object.__setattr__(self, 'retriever', retriever)

    def get_inputs(self) -> List[str]:
        """
        showing inputs for the prompt template being used
        Returns:
            List: A list of input variable, each one should be str
        """
        return self.prompt.input_variables
