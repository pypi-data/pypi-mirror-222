{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format inputs to Chat models\n",
    "\n",
    "Easyllm can be used as an abstract layer to replace `gpt-3.5-turbo` and `gpt-4` with open source models.\n",
    "\n",
    "You can change your own applications from the OpenAI API, by simply changing the client. \n",
    "\n",
    "Chat models take a series of messages as input, and return an AI-written message as output.\n",
    "\n",
    "This guide illustrates the chat format with a few example API calls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the openai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade easyllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the EasyLLM Python library for calling the EasyLLM API\n",
    "import easyllm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An example chat API call\n",
    "\n",
    "A chat API call has two required inputs:\n",
    "- `model`: the name of the model you want to use (e.g., `meta-llama/Llama-2-70b-chat-hf`) or leave it empty to just call the api\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Compared to OpenAI api is the `huggingface` module also exposing a `prompt_builder` and `stop_sequences` parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. `llama2_prompt_builder` and `llama2_stop_sequences`. \n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from easyllm.clients import huggingface\n",
    "from easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt\n",
    "\n",
    "# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n",
    "# huggingface.api_key=\"hf_xxx\"\n",
    "\n",
    "MODEL = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "huggingface.prompt_builder = build_llama2_prompt\n",
    "\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    "      temperature=0.9,\n",
    "      top_p=0.6,\n",
    "      max_tokens=1024,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Orange who?\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/29/2023 21:13:43 - DEBUG - easyllm.utils - Prompt sent to model will be:\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n",
      "07/29/2023 21:13:43 - DEBUG - easyllm.utils - Url:\n",
      "https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n",
      "07/29/2023 21:13:43 - DEBUG - easyllm.utils - Stop sequences:\n",
      "[]\n",
      "07/29/2023 21:13:43 - DEBUG - easyllm.utils - Generation parameters:\n",
      "{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.2, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n",
      "07/29/2023 21:13:43 - DEBUG - easyllm.utils - Response at index 0:\n",
      "index=0 message=ChatMessage(role='assistant', content=' Ahoy matey! Ol\\' Blackbeard here be explainin\\' asynchronous programming, arrr!\\n\\nAsynchronous programming be like sailin\\' a ship, ye see. Ye got yer crew, and each member be doin\\' their own task, like hoistin\\' the sails, swabbin\\' the deck, or mannin\\' the cannons. But, ye don\\'t be tellin\\' them what to do, they be doin\\' it on their own, in their own time, savvy?\\n\\nNow, imagine ye be the captain, and ye want to make sure the ship be sailin\\' smoothly. Ye give orders, but ye don\\'t be tellin\\' each crew member exactly when to do their task. Instead, ye be sayin\\', \"Alright me hearties, we need to hoist the sails, swab the deck, and man the cannons, but ye can do it in yer own time, just make sure it be done before we reach the next port!\"\\n\\nThat be asynchronous programming, me hearty! Ye be tellin\\' the crew what needs to be done, but not when to do it. They be figure out the best time to do their task, and they be doin\\' it on their own, without ye tellin\\' \\'em what to do every step o\\' the way.\\n\\nBut, arrr, there be a catch, me matey! If ye not careful, the crew might be doin\\' their tasks at the wrong time, and the ship might not sail smoothly. That be why ye need to be careful when ye be writin\\' yer code, and make sure that each task be done in the right order, and that they be done before the next task can be started.\\n\\nSo, hoist the sails, me hearties, and set sail fer asynchronous programming! It be a powerful tool, but ye need to use it wisely, or ye might find yerself walkin\\' the plank! Arrr!') finish_reason=None\n",
      " Ahoy matey! Ol' Blackbeard here be explainin' asynchronous programming, arrr!\n",
      "\n",
      "Asynchronous programming be like sailin' a ship, ye see. Ye got yer crew, and each member be doin' their own task, like hoistin' the sails, swabbin' the deck, or mannin' the cannons. But, ye don't be tellin' them what to do, they be doin' it on their own, in their own time, savvy?\n",
      "\n",
      "Now, imagine ye be the captain, and ye want to make sure the ship be sailin' smoothly. Ye give orders, but ye don't be tellin' each crew member exactly when to do their task. Instead, ye be sayin', \"Alright me hearties, we need to hoist the sails, swab the deck, and man the cannons, but ye can do it in yer own time, just make sure it be done before we reach the next port!\"\n",
      "\n",
      "That be asynchronous programming, me hearty! Ye be tellin' the crew what needs to be done, but not when to do it. They be figure out the best time to do their task, and they be doin' it on their own, without ye tellin' 'em what to do every step o' the way.\n",
      "\n",
      "But, arrr, there be a catch, me matey! If ye not careful, the crew might be doin' their tasks at the wrong time, and the ship might not sail smoothly. That be why ye need to be careful when ye be writin' yer code, and make sure that each task be done in the right order, and that they be done before the next task can be started.\n",
      "\n",
      "So, hoist the sails, me hearties, and set sail fer asynchronous programming! It be a powerful tool, but ye need to use it wisely, or ye might find yerself walkin' the plank! Arrr!\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/29/2023 21:11:56 - DEBUG - easyllm.utils - Prompt sent to model will be:\n",
      "<s>[INST] Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n",
      "07/29/2023 21:11:56 - DEBUG - easyllm.utils - Url:\n",
      "https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n",
      "07/29/2023 21:11:56 - DEBUG - easyllm.utils - Stop sequences:\n",
      "[]\n",
      "07/29/2023 21:11:56 - DEBUG - easyllm.utils - Generation parameters:\n",
      "{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n",
      "07/29/2023 21:11:56 - DEBUG - easyllm.utils - Response at index 0:\n",
      "index=0 message=ChatMessage(role='assistant', content=' Ahoy matey! Yer lookin\\' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o\\' grog and listen close, for Blackbeard\\'s got a story fer ye.\\n\\nAsynchronous programming, me hearties, be like sailin\\' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\\n\\nImagine ye\\'re sailin\\' along, and suddenly, out o\\' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye\\'ll be sent to Davy Jones\\' locker!\\n\\nBut, me hearties, ye can\\'t just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that\\'s where asynchronous programming comes in.\\n\\nAsynchronous programming be like haulin\\' up the sails, and lettin\\' the wind do the work fer ye. Ye don\\'t have to worry about the details o\\' how the wind\\'s blowin\\', or the waves crashin\\', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\\n\\nNow, I know what ye\\'re thinkin\\', \"Blackbeard, how do I know when me ship\\'s gonna make it through the storm?\" And that, me hearties, be the beauty o\\' asynchronous programming. Ye don\\'t have to know! Ye just have to trust that the winds o\\' change will carry ye through, and ye\\'ll make it to the other side, all in one piece.\\n\\nBut, me hearties, don\\'t ye be thinkin\\' this be easy. Asynchronous programming be like navigatin\\' through treacherous waters, with a crew o\\' mutinous code, and a hull full o\\' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye\\'ll be walkin\\' the plank!\\n\\nSo, me hearties, there ye have it. Asynchronous programming in the style o\\' Blackbeard. May the winds o\\' change blow in yer favor, and may yer code always be free o\\' bugs! Arrr!') finish_reason=None\n",
      " Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\n",
      "\n",
      "Asynchronous programming, me hearties, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the hidden dangers that lie beneath the surface.\n",
      "\n",
      "Imagine ye're sailin' along, and suddenly, out o' the blue, a great storm brews up. The winds howl, the waves crash, and yer ship takes on water. Now, ye gotta act fast, or ye'll be sent to Davy Jones' locker!\n",
      "\n",
      "But, me hearties, ye can't just abandon ship. Ye gotta batten down the hatches, and ride out the storm. And that's where asynchronous programming comes in.\n",
      "\n",
      "Asynchronous programming be like haulin' up the sails, and lettin' the wind do the work fer ye. Ye don't have to worry about the details o' how the wind's blowin', or the waves crashin', ye just gotta keep yer ship pointed in the right direction, and let nature take its course.\n",
      "\n",
      "Now, I know what ye're thinkin', \"Blackbeard, how do I know when me ship's gonna make it through the storm?\" And that, me hearties, be the beauty o' asynchronous programming. Ye don't have to know! Ye just have to trust that the winds o' change will carry ye through, and ye'll make it to the other side, all in one piece.\n",
      "\n",
      "But, me hearties, don't ye be thinkin' this be easy. Asynchronous programming be like navigatin' through treacherous waters, with a crew o' mutinous code, and a hull full o' bugs. Ye gotta be prepared fer the unexpected, and have a stout heart, or ye'll be walkin' the plank!\n",
      "\n",
      "So, me hearties, there ye have it. Asynchronous programming in the style o' Blackbeard. May the winds o' change blow in yer favor, and may yer code always be free o' bugs! Arrr!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message and debug flag on:\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We've changed direction too late to do a complete job for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
