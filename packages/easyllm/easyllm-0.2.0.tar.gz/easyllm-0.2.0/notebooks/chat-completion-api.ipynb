{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to format inputs to Chat models\n",
    "\n",
    "Easyllm can be used as an abstract layer to replace `gpt-3.5-turbo` and `gpt-4` with open source models.\n",
    "\n",
    "You can change your own applications from the OpenAI API, by simply changing the client. \n",
    "\n",
    "Chat models take a series of messages as input, and return an AI-written message as output.\n",
    "\n",
    "This guide illustrates the chat format with a few example API calls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the easyllm library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade easyllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the EasyLLM Python library for calling the EasyLLM API\n",
    "import easyllm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An example chat API call\n",
    "\n",
    "A chat API call has two required inputs:\n",
    "- `model`: the name of the model you want to use (e.g., `meta-llama/Llama-2-70b-chat-hf`) or leave it empty to just call the api\n",
    "- `messages`: a list of message objects, where each object has two required fields:\n",
    "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
    "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
    "\n",
    "Compared to OpenAI api is the `huggingface` module also exposing a `prompt_builder` and `stop_sequences` parameter you can use to customize the prompt and stop sequences. The EasyLLM package comes with build in popular methods for both of these parameters, e.g. `llama2_prompt_builder` and `llama2_stop_sequences`. \n",
    "\n",
    "Let's look at an example chat API calls to see how the chat format works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'hf-VH-mBO0hVT',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1690987326,\n",
       " 'model': 'meta-llama/Llama-2-70b-chat-hf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant', 'content': ' Apple who?'},\n",
       "   'finish_reason': 'eos_token'}],\n",
       " 'usage': {'prompt_tokens': 149, 'completion_tokens': 5, 'total_tokens': 154}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from easyllm.clients import huggingface\n",
    "from easyllm.prompt_utils import llama2_stop_sequences, build_llama2_prompt\n",
    "# Example EasyLLM Python library request\n",
    "MODEL = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "huggingface.prompt_builder = build_llama2_prompt\n",
    "\n",
    "# The module automatically loads the HuggingFace API key from the environment variable HUGGINGFACE_TOKEN or from the HuggingFace CLI configuration file.\n",
    "# huggingface.api_key=\"hf_xxx\"\n",
    "\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Apple.\"},\n",
    "    ],\n",
    "      temperature=0.9,\n",
    "      top_p=0.6,\n",
    "      max_tokens=1024,\n",
    ")\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response object has a few fields:\n",
    "- `id`: the ID of the request\n",
    "- `object`: the type of object returned (e.g., `chat.completion`)\n",
    "- `created`: the timestamp of the request\n",
    "- `model`: the full name of the model used to generate the response\n",
    "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
    "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
    "    - `message`: the message object generated by the model, with `role` and `content`\n",
    "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
    "    - `index`: the index of the completion in the list of choices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the reply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apple who?\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
    "\n",
    "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello, my dear students! Today, we're going to learn about a fascinating topic that will help us understand how to make our programs more efficient and responsive: asynchronous programming.\n",
      "\n",
      "Imagine you're working on a project with your classmates, and you need to finish a task, but you're waiting for someone else to finish their part first. You can't start your work until they're done, but you don't want to just sit there twiddling your thumbs. That's where asynchronous programming comes in!\n",
      "\n",
      "Asynchronous programming is like working on a project with your classmates, but instead of waiting for them to finish, you can start working on something else in the meantime. You can think of it like this:\n",
      "\n",
      "1. You give a task to your classmate, and they start working on it.\n",
      "2. While they're working, you start working on another task that doesn't depend on their work.\n",
      "3. When your classmate finishes their task, they give it back to you, and you can continue working on your task.\n",
      "\n",
      "This way, you can work on multiple tasks simultaneously, and you don't have to wait for each other to finish before moving on to the next step. It's like having multiple people working on a project at the same time, but they're all working on different parts of it.\n",
      "\n",
      "Now, let's see how this works in programming. Imagine you're building a website, and you want to load some data from an API. You can't start building the website until the data is loaded, but you don't want to wait for the data to be loaded before you start building the website. That's where asynchronous programming comes in!\n",
      "\n",
      "You can use a special function called a \"callback\" to handle this situation. A callback is like a homework assignment that you give to a classmate. You give them the assignment, and they work on it. When they're done, they give it back to you, and you can continue working on your project.\n",
      "\n",
      "Here's an example of how this might look in code:\n",
      "```\n",
      "// Give the homework assignment (callback) to the API\n",
      "fetchDataFromApi(callback);\n",
      "\n",
      "// Start working on the website while waiting for the data to be loaded\n",
      "buildWebsite();\n",
      "\n",
      "// When the data is loaded, continue working on the website\n",
      "function callback(data) {\n",
      "  // Use the data to finish building the website\n",
      "  buildWebsiteWithData(data);\n",
      "}\n",
      "```\n",
      "Asynchronous programming can be a bit tricky to wrap your head around at first, but once you understand the concept, it's a powerful tool for building efficient and responsive programs. So, keep practicing, and soon you'll be a master of asynchronous programming!\n"
     ]
    }
   ],
   "source": [
    "# example with a system message\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of math teacher.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/02/2023 16:42:33 - DEBUG - easyllm.utils - Prompt sent to model will be:\n",
      "<s>[INST] Explain asynchronous programming in the style of the pirate Blackbeard. [/INST]\n",
      "08/02/2023 16:42:33 - DEBUG - easyllm.utils - Url:\n",
      "https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n",
      "08/02/2023 16:42:33 - DEBUG - easyllm.utils - Stop sequences:\n",
      "[]\n",
      "08/02/2023 16:42:33 - DEBUG - easyllm.utils - Generation parameters:\n",
      "{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n",
      "08/02/2023 16:42:58 - DEBUG - easyllm.utils - Response at index 0:\n",
      "index=0 message=ChatMessage(role='assistant', content=\" Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\\n\\nAsynchronous programming, me hearty, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the rocks and shoals that'll sink ye ship if ye ain't careful.\\n\\nImagine ye're sailin' along, and ye need to send a message to another ship, say, to arrange a meetin' at the next port o' call. Now, ye could just shout out the message and hope the other ship hears ye, but that be a risky business. The wind and the waves might carry yer words away, and the other ship might not hear ye at all.\\n\\nOr, ye could use a bit o' magic, like a bottle with a message inside, and throw it into the sea. The currents and the tides'll carry it to the other ship, and they'll find it when they least expect it. That be asynchronous programming, me hearty!\\n\\nYe see, when ye send a message, ye don't know when it'll arrive, or even if it'll arrive at all. It's like sendin' a ship's boy up the riggin' to fetch a sail that's stuck. Ye don't know when he'll get there, but ye trust that he'll do his best to fetch it down.\\n\\nAnd that's where the magic comes in, me hearty. Ye gotta have faith that the message'll get there, and that the other ship'll receive it in good time. And while ye wait, ye can keep sailin' on, doin' other tasks, like fixin' the riggin' or swabbin' the decks.\\n\\nNow, there be times when the message don't arrive at all, or it arrives too late. That be like a storm blowin' in, and ye gotta adjust yer sails to keep the ship afloat. But that's the way o' the sea, me hearty. Ye gotta be ready fer anythin', and keep yer wits about ye at all times.\\n\\nSo there ye have it, me hearty. Asynchronous programming be like sailin' the high seas, with a bit o' magic and a lot o' faith. Keep yer wits about ye, and ye'll navigate the treacherous waters like a seasoned pirate! Arrr!\") finish_reason='eos_token'\n",
      " Ahoy matey! Yer lookin' fer a tale of asynchronous programming, eh? Well, settle yerself down with a pint o' grog and listen close, for Blackbeard's got a story fer ye.\n",
      "\n",
      "Asynchronous programming, me hearty, be like sailin' a ship through treacherous waters. Ye gotta keep yer wits about ye, and watch out fer the rocks and shoals that'll sink ye ship if ye ain't careful.\n",
      "\n",
      "Imagine ye're sailin' along, and ye need to send a message to another ship, say, to arrange a meetin' at the next port o' call. Now, ye could just shout out the message and hope the other ship hears ye, but that be a risky business. The wind and the waves might carry yer words away, and the other ship might not hear ye at all.\n",
      "\n",
      "Or, ye could use a bit o' magic, like a bottle with a message inside, and throw it into the sea. The currents and the tides'll carry it to the other ship, and they'll find it when they least expect it. That be asynchronous programming, me hearty!\n",
      "\n",
      "Ye see, when ye send a message, ye don't know when it'll arrive, or even if it'll arrive at all. It's like sendin' a ship's boy up the riggin' to fetch a sail that's stuck. Ye don't know when he'll get there, but ye trust that he'll do his best to fetch it down.\n",
      "\n",
      "And that's where the magic comes in, me hearty. Ye gotta have faith that the message'll get there, and that the other ship'll receive it in good time. And while ye wait, ye can keep sailin' on, doin' other tasks, like fixin' the riggin' or swabbin' the decks.\n",
      "\n",
      "Now, there be times when the message don't arrive at all, or it arrives too late. That be like a storm blowin' in, and ye gotta adjust yer sails to keep the ship afloat. But that's the way o' the sea, me hearty. Ye gotta be ready fer anythin', and keep yer wits about ye at all times.\n",
      "\n",
      "So there ye have it, me hearty. Asynchronous programming be like sailin' the high seas, with a bit o' magic and a lot o' faith. Keep yer wits about ye, and ye'll navigate the treacherous waters like a seasoned pirate! Arrr!\n"
     ]
    }
   ],
   "source": [
    "# example without a system message and debug flag on:\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
    "    ],\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Few-shot prompting\n",
    "\n",
    "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
    "\n",
    "One way to show the model what you want is with faked example messages.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/02/2023 16:42:58 - DEBUG - easyllm.utils - Prompt sent to model will be:\n",
      "<s>[INST] <<SYS>>\n",
      "You are a helpful, pattern-following assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Help me translate the following corporate jargon into plain English. [/INST] Sure, I'd be happy to!</s><s>[INST] New synergies will help drive top-line growth. [/INST] Things working well together will increase revenue.</s><s>[INST] Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage. [/INST] Let's talk later when we're less busy about how to do better.</s><s>[INST] This late pivot means we don't have time to boil the ocean for the client deliverable. [/INST]\n",
      "08/02/2023 16:42:58 - DEBUG - easyllm.utils - Url:\n",
      "https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\n",
      "08/02/2023 16:42:58 - DEBUG - easyllm.utils - Stop sequences:\n",
      "[]\n",
      "08/02/2023 16:42:58 - DEBUG - easyllm.utils - Generation parameters:\n",
      "{'do_sample': True, 'return_full_text': False, 'max_new_tokens': 1024, 'top_p': 0.6, 'temperature': 0.9, 'stop_sequences': [], 'repetition_penalty': 1.0, 'top_k': 10, 'seed': 42}\n",
      "08/02/2023 16:42:59 - DEBUG - easyllm.utils - Response at index 0:\n",
      "index=0 message=ChatMessage(role='assistant', content=\" We've changed direction too late to do a complete job for the client.\") finish_reason='eos_token'\n",
      " We've changed direction too late to do a complete job for the client.\n"
     ]
    }
   ],
   "source": [
    "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
    "response = huggingface.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every attempt at engineering conversations will succeed at first.\n",
    "\n",
    "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
    "\n",
    "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
    "\n",
    "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
